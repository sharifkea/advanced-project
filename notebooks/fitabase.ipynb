{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the heart_rate from dataset\n",
    "df = pd.read_csv('heartrate_seconds_merged.csv', parse_dates=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of entries for each Id\n",
    "id_counts = df['Id'].value_counts()\n",
    "print(id_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# For Id=4020332650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only 'Time' and 'Value' columns for Id = 4020332650\n",
    "#first_id = df['Id'].iloc[0]\n",
    "df_max_id = df[df['Id'] == 4020332650][['Time', 'Value']]\n",
    "df_max_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_max_id.set_index('Time')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 1-minute intervals, taking the mean value for each minute\n",
    "df_min = df.resample('1T').mean(numeric_only=True)\n",
    "\n",
    "# Check the result\n",
    "df_min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min.plot(style='.',\n",
    "        figsize=(15, 5),\n",
    "        color=color_pal[0],\n",
    "        title='HR of first user over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values by interpolation\n",
    "df_min_filled = df_min['Value'].interpolate()\n",
    "\n",
    "# Or, to simply drop missing values (may affect regularity)\n",
    "# df_min_filled = df_min['Value'].dropna()\n",
    "\n",
    "result = seasonal_decompose(df_min_filled, model='additive', period=60*24)\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Filling missing values with the average value for that minute across all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure 'Time' is the index and is datetime type\n",
    "df_sorted = df_max_id.set_index('Time')\n",
    "df_sorted.index = pd.to_datetime(df_sorted.index)\n",
    "\n",
    "# Resample to 1-minute intervals (creates missing minutes as NaN)\n",
    "df_min = df_sorted.resample('1T').mean(numeric_only=True)\n",
    "\n",
    "# Extract minute of day for each timestamp\n",
    "df_min['minute_of_day'] = df_min.index.hour * 60 + df_min.index.minute\n",
    "\n",
    "# Compute average value for each minute of the day (across all days)\n",
    "minute_avg = df_min.groupby('minute_of_day')['Value'].mean()\n",
    "\n",
    "# Fill missing values with the average for that minute of the day\n",
    "df_min['Value'] = df_min.apply(\n",
    "    lambda row: minute_avg[row['minute_of_day']] if pd.isna(row['Value']) else row['Value'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Now df_min['Value'] has no missing values, and each missing minute is filled with its typical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min['Value'].plot(\n",
    "    style='.',\n",
    "    figsize=(15, 5),\n",
    "    color=color_pal[0],\n",
    "    title='HR of first user over time'\n",
    ")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Heart Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values by interpolation\n",
    "df_min_filled = df_min['Value'].interpolate()\n",
    "\n",
    "# Or, to simply drop missing values (may affect regularity)\n",
    "# df_min_filled = df_min['Value'].dropna()\n",
    "\n",
    "result = seasonal_decompose(df_min_filled, model='additive', period=60*12)\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "* # Train Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_min_filled.to_frame()\n",
    "train = df.loc[df.index < '2016-04-10']\n",
    "test = df.loc[df.index >= '2016-04-10']\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "train.plot(ax=ax, label='Training Set', title='Data Train/Test Split')\n",
    "test.plot(ax=ax, label='Test Set')\n",
    "ax.axvline('10-04-2015', color='black', ls='--')\n",
    "ax.legend(['Training Set', 'Test Set'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features and lag features based on time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Basic time-based features\n",
    "    df['minute'] = df.index.minute\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day'] = df.index.day\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "\n",
    "    # Lag features\n",
    "    df['lag_1minute'] = df['Value'].shift(1)  # 1 minute lag\n",
    "    df['lag_1h'] = df['Value'].shift(60)   # 1 hour lag\n",
    "    df['lag_1d'] = df['Value'].shift(1440)  # 1 day lag\n",
    "    df['lag_1w'] = df['Value'].shift(10080) # 1 week lag\n",
    "\n",
    "    # Rolling statistics features\n",
    "    df['rolling_mean_30minutes'] = df['Value'].rolling(window=30).mean()  # Last 30 minutes rolling mean\n",
    "    df['rolling_mean_3hours'] = df['Value'].rolling(window=180).mean()  # Last 3 hours rolling mean\n",
    "    df['rolling_mean_3days'] = df['Value'].rolling(window=4320).mean()  # Last 3 days rolling mean\n",
    "    df['rolling_mean_same_hour_last_day'] = df['Value'].shift(1440).rolling(window=30).mean()  # Same hour previous day rolling mean\n",
    "    df['rolling_mean_same_hour_last_week'] = df['Value'].shift(10080).rolling(window=7).mean()  # Same hour previous week rolling mean\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_features(df_min_filled.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## **Visualization Feature/Target Relationship**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.boxplot(data=df, x='hour', y='Value', ax=ax)\n",
    "ax.set_title('Heart Rate by Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.boxplot(data=df, x='dayofweek', y='Value', palette='Blues')\n",
    "ax.set_title('Heart Rate by Day of Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## **Preparing Data For Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_features(train)\n",
    "test = create_features(test)\n",
    "\n",
    "\n",
    "TARGET = 'Value'\n",
    "\n",
    "\n",
    "FEATURES_XGB = [\n",
    "    'hour', 'dayofweek', 'month', 'minute', 'day','lag_1minute', 'lag_1h', 'lag_1d', 'lag_1w',\n",
    "    'rolling_mean_30minutes', 'rolling_mean_3hours', 'rolling_mean_3days', 'rolling_mean_same_hour_last_day', 'rolling_mean_same_hour_last_week'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Data\n",
    "X_train_xgb = train[FEATURES_XGB]\n",
    "y_train_xgb = train[TARGET]\n",
    "\n",
    "X_test_xgb = test[FEATURES_XGB]\n",
    "y_test_xgb = test[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "dtest = xgb.DMatrix(X_test_xgb, label=y_test_xgb)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Objective function for regression\n",
    "    'eval_metric': 'rmse',  # Evaluation metric\n",
    "    'max_depth': 3,  # Depth of the trees\n",
    "    'learning_rate': 0.01,  # Learning rate\n",
    "    'colsample_bytree': 0.8,  # Subsample of features\n",
    "    'subsample': 0.8  # Subsample ratio\n",
    "}\n",
    "\n",
    "# Watchlist for monitoring performance on train and test sets\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "# Number of boosting rounds and early stopping\n",
    "num_round = 1000  # Number of boosting rounds\n",
    "early_stopping_rounds = 50  # Early stopping\n",
    "\n",
    "# Train the model\n",
    "reg = xgb.train(params, dtrain, num_round, watchlist, early_stopping_rounds=early_stopping_rounds,verbose_eval=100)\n",
    "\n",
    "# # Predict on the test set\n",
    "y_pred_xgb = reg.predict(dtest)\n",
    "\n",
    "# # Evaluate the performance (optional, to see RMSE)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "# print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that 'prediction' column is added only once\n",
    "if 'prediction' not in df.columns:\n",
    "    test['prediction'] = reg.predict(dtest)\n",
    "    df = df.merge(test[['prediction']], how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Plot the actual and predicted values\n",
    "ax = df[['Value']].plot(figsize=(15, 5))\n",
    "df['prediction'].plot(ax=ax, style='.', color='orange')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.legend(['Actual Data', 'Predictions'])\n",
    "ax.set_title('Actual Data vs Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Heart Rate')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title **Evaluating XGBoost**\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test_xgb, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost - RMSE: {rmse_xgb}\")\n",
    "print(f\"XGBoost - MAE: {mae_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Making predictoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the target time\n",
    "target_time = pd.to_datetime('2016-04-13 01:03:00')\n",
    "\n",
    "# 2. Create a new row with the correct index\n",
    "row = pd.DataFrame(index=[target_time])\n",
    "\n",
    "# 3. Concatenate this row to your df to ensure all lags/rolling can be calculated\n",
    "df_with_row = pd.concat([df, row])\n",
    "df_with_row = df_with_row.sort_index()\n",
    "\n",
    "# 4. Create features for the new row (this will use previous data for lags/rolling)\n",
    "df_with_row = create_features(df_with_row)\n",
    "\n",
    "# 5. Select the last row (the one for prediction)\n",
    "X_row = df_with_row.loc[[target_time], FEATURES_XGB]\n",
    "\n",
    "# 6. Predict using the trained model\n",
    "drow = xgb.DMatrix(X_row)\n",
    "predicted_hr = reg.predict(drow)[0]\n",
    "\n",
    "print(f\"Predicted HR at {target_time}: {predicted_hr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 1. Prepare data\n",
    "X_train = train[FEATURES_XGB].values\n",
    "y_train = train[TARGET].values\n",
    "X_test = test[FEATURES_XGB].values\n",
    "y_test = test[TARGET].values\n",
    "\n",
    "# 2. Build the model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 3. Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred_nn = model.predict(X_test).flatten()\n",
    "\n",
    "# 5. Evaluate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "print(f\"Neural Network - RMSE: {rmse_nn}\")\n",
    "print(f\"Neural Network - MAE: {mae_nn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "FEATURES_XGB = [\n",
    "    'hour', 'dayofweek', 'month', 'minute', 'day'\n",
    "]\n",
    "\n",
    "# 1. Prepare data for LSTM\n",
    "window_size = 60  # Use last 60 minutes\n",
    "features = FEATURES_XGB  # Use your feature list\n",
    "\n",
    "def create_lstm_dataset(df, features, window_size):\n",
    "    X, y = [], []\n",
    "    values = df[features].values\n",
    "    targets = df[TARGET].values\n",
    "    for i in range(window_size, len(df)):\n",
    "        X.append(values[i-window_size:i])\n",
    "        y.append(targets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create LSTM datasets for train and test\n",
    "X_train_lstm, y_train_lstm = create_lstm_dataset(train, features, window_size)\n",
    "X_test_lstm, y_test_lstm = create_lstm_dataset(test, features, window_size)\n",
    "\n",
    "# 2. Build the LSTM model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(units=128, input_shape=(window_size, len(features)), return_sequences=True),\n",
    "    keras.layers.LSTM(units=64),  # Second LSTM layer\n",
    "    keras.layers.Dense(units=1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 3. Train the model\n",
    "model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred_lstm = model.predict(X_test_lstm).flatten()\n",
    "\n",
    "# 5. Evaluate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm, y_pred_lstm))\n",
    "mae_lstm = mean_absolute_error(y_test_lstm, y_pred_lstm)\n",
    "print(f\"LSTM - RMSE: {rmse_lstm}\")\n",
    "print(f\"LSTM - MAE: {mae_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all timestamps are present\n",
    "expected_index = pd.date_range(start=target_time - pd.Timedelta(minutes=window_size), \n",
    "                              end=target_time - pd.Timedelta(minutes=1), freq='T')\n",
    "missing = set(expected_index) - set(df.index)\n",
    "print(\"Missing timestamps:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = df.loc[input_start:input_end, FEATURES_XGB]\n",
    "print(input_seq.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure regular time index\n",
    "df = df.asfreq('T')  # Fills in missing minutes with NaN\n",
    "df[FEATURES_XGB] = df[FEATURES_XGB].interpolate()  # Interpolate missing feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Set the target time and window size\n",
    "target_time = pd.to_datetime('2016-04-13 01:03:00')\n",
    "window_size = 60  # Should match your LSTM training\n",
    "\n",
    "# 2. Get the previous window_size rows ending just before the target time\n",
    "input_end = target_time - pd.Timedelta(minutes=1)\n",
    "input_start = input_end - pd.Timedelta(minutes=window_size-1)\n",
    "input_seq = df.loc[input_start:input_end, FEATURES_XGB].values\n",
    "\n",
    "# 3. Check shape and reshape for LSTM input\n",
    "if input_seq.shape[0] == window_size:\n",
    "    input_seq = input_seq.reshape(1, window_size, len(FEATURES_XGB))\n",
    "    # 4. Predict\n",
    "    predicted_hr = model.predict(input_seq)[0, 0]\n",
    "    print(f\"LSTM predicted HR at {target_time}: {predicted_hr:.2f}\")\n",
    "else:\n",
    "    print(\"Not enough data to create input sequence for prediction.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
